{
  "metadata": {
    "name": "SparkKafkaStreaming",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nval brokers \u003d \"localhost:9092\"   // \u003c--- IP 설정  (1)\nval topics \u003d Array(\"mysql_source_json_no_schema2_users\")       //  \u003c--- topic 이름 설정 (2)\nval ssc \u003d new StreamingContext(spark.sparkContext, Seconds(1))\n\nval kafkaParams \u003d Map[String, Object](\n\"bootstrap.servers\" -\u003e brokers,\n\"key.deserializer\" -\u003e classOf[StringDeserializer],\n\"value.deserializer\" -\u003e classOf[StringDeserializer],\n\"group.id\" -\u003e \"use_a_separate_group_id_for_each_stream\",\n\"auto.offset.reset\" -\u003e \"earliest\",\n\"enable.auto.commit\" -\u003e (false: java.lang.Boolean)\n)\n\n\nval stream \u003d KafkaUtils.createDirectStream[String, String](\nssc,\nPreferConsistent,\nSubscribe[String, String](topics, kafkaParams)\n)\n\nstream.map(record \u003d\u003e record.value()).print();\n\nssc.start()\nssc.awaitTermination()"
    }
  ]
}
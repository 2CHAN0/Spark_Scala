{
  "metadata": {
    "name": "SparkOrcMerge",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%\nimport org.apache.hadoop.fs.{FileSystem, Path}\nval fs \u003d FileSystem.get(spark.sparkContext.hadoopConfiguration)\n//1. read files \nval fileList  \u003d fs.listStatus(new Path(\"hdfs:///topics/mysql_source_json_schema_users/partition\u003d0\"))\nvar fileNames \u003d Seq()\nfileList.foreach(x \u003d\u003e fileNames+x.getPath.toString)\n//2. spark read files\nval data \u003d spark.read.format(\"orc\").load(\"/topics/mysql_source_json_schema_users/partition\u003d0/*.orc\")\n//3. repartition to 1 -\u003e partitionByKey\ndata.repartition(1).write.mode(\"overwrite\").partitionBy(\"age\").orc(\"/merge_test/test04\")\n//4. remove small files\nfileList.foreach( x \u003d\u003e fs.delete(new Path(x.getPath.toString), true) )"
    }
  ]
}